{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b63825-5b10-4354-8c95-36ce9facda05",
   "metadata": {},
   "source": [
    "Team 10 NLP Project\n",
    "\n",
    "Dataset: One Million Post Corpus\n",
    "Dataset Language: German\n",
    "\n",
    "Goal: Classify and predict User-Post catgeory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc44edd-4ea6-4347-b6ac-0950192d01d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID_Post  ID_Annotator         Category  Value\n",
      "0     3326             1    ArgumentsUsed      0\n",
      "1     3326             1   Discriminating      0\n",
      "2     3326             1    Inappropriate      0\n",
      "3     3326             1         OffTopic      0\n",
      "4     3326             1  PersonalStories      0\n",
      "   ID_Post          Category  Value\n",
      "0       79  PossiblyFeedback      1\n",
      "1       81  PossiblyFeedback      1\n",
      "2      132  PossiblyFeedback      1\n",
      "3      134  PossiblyFeedback      1\n",
      "4      139  PossiblyFeedback      1\n",
      "   ID_Article                              Path          publishingDate  \\\n",
      "0           1           Newsroom/User/Community  2012-05-26 03:00:19.23   \n",
      "1           2    Newsroom/User/Community/Regeln  2012-05-26 12:12:19.46   \n",
      "2           3                    Diverses/mobil  2013-11-22 12:15:00.00   \n",
      "3           4  Newsroom/User/mitmachen/Mitreden  2014-08-13 05:30:00.00   \n",
      "4           5  Newsroom/User/mitmachen/Mitreden  2014-08-27 12:27:01.09   \n",
      "\n",
      "                                               Title  \\\n",
      "0                  Die Newsletter von derStandard.at   \n",
      "1                Werden Sie Teil von derStandard.at!   \n",
      "2                 Die Android App von derStandard.at   \n",
      "3  Welche Erfahrungen haben Sie als Linkshänder g...   \n",
      "4                Wie haben Sie das Jahr 1989 erlebt?   \n",
      "\n",
      "                                                Body  \n",
      "0  <div class=\"section\" id=\"content-main\" itempro...  \n",
      "1  <div class=\"diashow\" id=\"objectContent\"><meta ...  \n",
      "2  <div class=\"section\" id=\"content-main\" itempro...  \n",
      "3  <div class=\"section\" id=\"content-main\" itempro...  \n",
      "4  <div class=\"section\" id=\"content-main\" itempro...  \n",
      "                Name  Ord\n",
      "0  SentimentNegative    1\n",
      "1   SentimentNeutral    2\n",
      "2  SentimentPositive    3\n",
      "3           OffTopic    4\n",
      "4      Inappropriate    5\n",
      "   ID_Post          Category  Fold\n",
      "0       79  PossiblyFeedback     1\n",
      "1       81  PossiblyFeedback     1\n",
      "2      132  PossiblyFeedback     1\n",
      "3      134  PossiblyFeedback     1\n",
      "4      139  PossiblyFeedback     1\n",
      "   ID_User\n",
      "0      123\n",
      "1      383\n",
      "2      407\n",
      "3      461\n",
      "4      497\n",
      "   ID_Post  ID_Parent_Post  ID_Article  ID_User                CreatedAt  \\\n",
      "0        1             NaN           1     9089  2003-04-23 14:52:41.870   \n",
      "1        2             NaN           1    29367  2003-11-04 16:21:57.850   \n",
      "2        3             2.0           1     5095  2004-01-28 12:57:28.240   \n",
      "3        4             3.0           1     1682  2004-02-03 20:32:39.123   \n",
      "4        5             NaN           1     3343  2004-03-02 11:37:44.100   \n",
      "\n",
      "    Status                   Headline  \\\n",
      "0  deleted                              \n",
      "1   online  Newsletter \"DER STANDARD\"   \n",
      "2  deleted         Auch begeistert...   \n",
      "3  deleted            Abmeldeprobleme   \n",
      "4   online                              \n",
      "\n",
      "                                                Body  PositiveVotes  \\\n",
      "0                                                                 0   \n",
      "1  Ich bin begeistert von den STANDARD - Newslett...              0   \n",
      "2  ... Aber momentan funktioniert das Abmelden od...              0   \n",
      "3  Es ist ganz einfach nervend!\\r\\nVor kurzem hab...              0   \n",
      "4   und sie als mitarbeiter sind natuerlich objektiv              0   \n",
      "\n",
      "   NegativeVotes  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "# Create your connection.\n",
    "cnx = sqlite3.connect('corpus.sqlite3')\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM Annotations\", cnx)\n",
    "print(df.head())\n",
    "\n",
    "df2 = pd.read_sql_query(\"SELECT * FROM Annotations_consolidated\", cnx)\n",
    "print(df2.head())\n",
    "\n",
    "df3 = pd.read_sql_query(\"SELECT * FROM Articles\", cnx)\n",
    "print(df3.head())\n",
    "\n",
    "df4 = pd.read_sql_query(\"SELECT * FROM Categories\", cnx)\n",
    "print(df4.head())\n",
    "\n",
    "df5 = pd.read_sql_query(\"SELECT * FROM CrossValSplit\", cnx)\n",
    "print(df5.head())\n",
    "\n",
    "df6 = pd.read_sql_query(\"SELECT * FROM Newspaper_Staff\", cnx)\n",
    "print(df6.head())\n",
    "\n",
    "df7 = pd.read_sql_query(\"SELECT * FROM Posts\", cnx)\n",
    "print(df7.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee76977c-59d5-4f2f-a8ea-4804cfbc5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "################### Create Posts_Annotated ###################\n",
    "##############################################################\n",
    "import sqlite3\n",
    "\n",
    "def getData(db_name):\n",
    "    # open and (if not exists) create database file\n",
    "    con = sqlite3.connect(db_name)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # create table\n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS Posts_Annotated (\" +\n",
    "                \"ID_Post           INTEGER PRIMARY KEY,\" +\n",
    "                \"Status            INTEGER,\" +\n",
    "                \"Headline          TEXT,\" +\n",
    "                \"Body              TEXT,\" +\n",
    "                \"PositiveVotes     INTEGER,\" +\n",
    "                \"NegativeVotes     INTEGER,\" +\n",
    "                \"ArgumentsUsed     INTEGER,\" +\n",
    "                \"Discriminating    INTEGER,\" +\n",
    "                \"Inappropriate     INTEGER,\" +\n",
    "                \"OffTopic          INTEGER,\" +\n",
    "                \"PersonalStories   INTEGER,\" +\n",
    "                \"PossiblyFeedback  INTEGER,\" +\n",
    "                \"SentimentNegative INTEGER,\" +\n",
    "                \"SentimentNeutral  INTEGER,\" +\n",
    "                \"SentimentPositive INTEGER\" +\n",
    "                \");\")\n",
    "    \n",
    "    # check if we need to add data\n",
    "    cur.execute('SELECT Count(*) FROM Posts_Annotated')\n",
    "    count = cur.fetchall()\n",
    "    if(count[0][0] > 1):\n",
    "        return\n",
    "\n",
    "    # insert statement\n",
    "    insert_table = \"INSERT INTO Posts_Annotated ( ID_Post, Status, Headline, Body, PositiveVotes, NegativeVotes, ArgumentsUsed, Discriminating, Inappropriate, OffTopic, PersonalStories, PossiblyFeedback, SentimentNegative, SentimentNeutral, SentimentPositive ) values ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )\"\n",
    "\n",
    "    # select annotation\n",
    "    get_annotation = \"SELECT Category, Value FROM Annotations_consolidated WHERE ID_Post = ? ORDER BY Category\"\n",
    "\n",
    "    # get all posts we need\n",
    "    cur.execute('SELECT ID_Post, ' +\n",
    "                'Status, ' +\n",
    "                'Headline, ' +\n",
    "                'Body, ' +\n",
    "                'PositiveVotes, ' +\n",
    "                'NegativeVotes ' +\n",
    "                'FROM ( ' +\n",
    "                'SELECT * ' +\n",
    "                'FROM Posts ' +\n",
    "                'WHERE ID_Post IN ( ' +\n",
    "                'SELECT ID_Post ' +\n",
    "                'FROM Annotations_consolidated ' +\n",
    "                'WHERE Category = \"ArgumentsUsed\"));')\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    for row in rows:\n",
    "        next_row = []\n",
    "\n",
    "        for row_data in row:\n",
    "            next_row.append(row_data)\n",
    "\n",
    "        # get annotations\n",
    "        cur.execute(get_annotation, [row[0]])\n",
    "        annotations = cur.fetchall()\n",
    "        for annotation in annotations:\n",
    "            next_row.append(annotation[1])\n",
    "\n",
    "        cur.execute(insert_table, next_row)\n",
    "\n",
    "    # Save (commit) the changes\n",
    "    con.commit()\n",
    "\n",
    "    # close the database connection\n",
    "    con.close()\n",
    "\n",
    "getData(\"corpus.sqlite3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c56932-3ef2-4be4-8870-d877adc43515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID_Post  Status                                      combined_text  \\\n",
      "0     3326  online       Top qualifizierte Leute verdienen auch viel    \n",
      "1     5321  online  Gott sei dank ist für sie eine Umfrage alles a...   \n",
      "2     5590  online  Die FPÖ wird aus allen Rohren schießen und die...   \n",
      "3     6015  online  Weil es dein meisten Leuten verständlicherweis...   \n",
      "4     8213  online                Na wer weis was da vorgefallen ist    \n",
      "\n",
      "   PositiveVotes  NegativeVotes  ArgumentsUsed  Discriminating  Inappropriate  \\\n",
      "0              1              3              0               0              0   \n",
      "1              2              1              1               0              0   \n",
      "2              7              1              1               0              0   \n",
      "3              2              0              0               0              1   \n",
      "4              0              0              0               0              0   \n",
      "\n",
      "   OffTopic  PersonalStories  PossiblyFeedback  SentimentNegative  \\\n",
      "0         0                0                 0                  0   \n",
      "1         0                0                 0                  1   \n",
      "2         0                0                 0                  0   \n",
      "3         0                0                 0                  1   \n",
      "4         0                0                 0                  0   \n",
      "\n",
      "   SentimentNeutral  SentimentPositive  \n",
      "0                 1                  0  \n",
      "1                 0                  0  \n",
      "2                 1                  0  \n",
      "3                 0                  0  \n",
      "4                 1                  0  \n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "############## Create Posts_Annotated_combined ###############\n",
    "##############################################################\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def remove_punct_and_numbers(corpus):\n",
    "    #code from https://stackoverflow.com/questions/45375488/how-to-filter-tokens-from-spacy-document\n",
    "    global nlp\n",
    "    indexes = []\n",
    "    doc = nlp(corpus)\n",
    "    for index, token in enumerate(doc):\n",
    "        if (token.pos_  in ('PUNCT', 'NUM', 'SYM')):\n",
    "            indexes.append(index)\n",
    "    np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
    "    np_array = np.delete(np_array, indexes, axis = 0)\n",
    "    doc2 = Doc(doc.vocab, words=[t.text for i, t in enumerate(doc) if i not in indexes])\n",
    "    doc2.from_array([LOWER, POS, ENT_TYPE, IS_ALPHA], np_array)\n",
    "    return doc2\n",
    "\n",
    "def combine_text(headline, body):\n",
    "    if(headline is None):\n",
    "        if(body is None):\n",
    "            return \"\"\n",
    "        else:\n",
    "            #return remove_punct_and_numbers(body)\n",
    "            return \"\".join(token.text_with_ws for token in remove_punct_and_numbers(body))\n",
    "    else:\n",
    "        if(body is None):\n",
    "            #return remove_punct_and_numbers(headline)\n",
    "            return \"\".join(token.text_with_ws for token in remove_punct_and_numbers(headline))\n",
    "        else:\n",
    "            #drei pünktchen entfernen und text ohne punkt \n",
    "            if headline[-3:] == \"...\":\n",
    "                headline = headline[:-3]\n",
    "            if body[:3] == \"...\":\n",
    "                body = body[3:]\n",
    "            #alternativ auch manchmal zwei pünktchen\n",
    "            if headline[-3:] == \"..\":\n",
    "                headline = headline[:-3]\n",
    "            if body[:3] == \"..\":\n",
    "                body = body[3:]\n",
    "            \n",
    "            #add missing space, to avoid combining words\n",
    "            if headline[-1:] != \" \":\n",
    "                if body[:1] != \" \":\n",
    "                    headline += \" \"\n",
    "            else:\n",
    "                if body[:1] == \" \":\n",
    "                    body = body[1:]\n",
    "                    \n",
    "            return \"\".join(token.text_with_ws for token in remove_punct_and_numbers(headline + body))\n",
    "\n",
    "con = sqlite3.connect('corpus.sqlite3')\n",
    "Posts_Annotated_df = pd.read_sql_query(\"SELECT * FROM Posts_Annotated\", con)\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "Posts_Annotated_df.insert(2, \"combined_text\", Posts_Annotated_df.apply(lambda x: combine_text(x[\"Headline\"], x[\"Body\"]), axis=1))\n",
    "Posts_Annotated_df.drop(columns=['Headline', 'Body'], inplace=True)\n",
    "Posts_Annotated_df.to_sql(\"Posts_Annotated_combined\", con, if_exists='fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00c20c7-fb60-4a96-8f7e-0629c04a35a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ArgumentsUsed\\nArgumentsUsed = np.empty((len(Posts_Annotated_df[\"ArgumentsUsed\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"ArgumentsUsed\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n    \\n# Discriminating\\nDiscriminating = np.empty((len(Posts_Annotated_df[\"Discriminating\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"Discriminating\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n\\n# Inappropriate\\nInappropriate = np.empty((len(Posts_Annotated_df[\"Inappropriate\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"Inappropriate\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n\\n# OffTopic\\nOffTopic = np.empty((len(Posts_Annotated_df[\"OffTopic\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"OffTopic\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n\\n# PersonalStories\\nPersonalStories = np.empty((len(Posts_Annotated_df[\"PersonalStories\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"PersonalStories\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n    \\n# PossiblyFeedback\\nPossiblyFeedback = np.empty((len(Posts_Annotated_df[\"PossiblyFeedback\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"PossiblyFeedback\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n    \\n# SentimentNegative\\nSentimentNegative = np.empty((len(Posts_Annotated_df[\"SentimentNegative\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"SentimentNegative\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n    \\n# SentimentNeutral\\nSentimentNeutral = np.empty((len(Posts_Annotated_df[\"SentimentNeutral\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"SentimentNeutral\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n\\n# SentimentPositive\\nSentimentPositive = np.empty((len(Posts_Annotated_df[\"SentimentPositive\"]), shape.vector.shape[0]))\\npos = 0\\nfor text in Posts_Annotated_df[\"SentimentPositive\"]:\\n    combined_text[pos] = nlp(text).vector\\n    pos += 1\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################################\n",
    "################# Create Dataframe for Test ##################\n",
    "##############################################################\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "con = sqlite3.connect('corpus.sqlite3')\n",
    "Posts_Annotated_df = pd.read_sql_query(\"SELECT * FROM Posts_Annotated_combined\", con)\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "shape = nlp(\"test\")\n",
    "\n",
    "#create vector representation for all inputs\n",
    "# Posts\n",
    "combined_text = np.empty((len(Posts_Annotated_df[\"combined_text\"]), shape.vector.shape[0]))\n",
    "pos = 0\n",
    "for text in Posts_Annotated_df[\"combined_text\"]:\n",
    "    combined_text[pos] = nlp(text).vector\n",
    "    pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4798e9ad-44de-4ea4-a84a-8ff738b83047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.53703703703703\n",
      "Recall: 63.01115241635687\n",
      "Precision: 58.752166377816295\n",
      "F1-Score: 60.80717488789238\n",
      "Confusion Matrix: [[304 238]\n",
      " [199 339]]\n",
      "Accuracy: 98.24074074074073\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1-Score: 0.0\n",
      "Confusion Matrix: [[1061    5]\n",
      " [  14    0]]\n",
      "Accuracy: 60.18518518518518\n",
      "Recall: 65.79925650557621\n",
      "Precision: 59.0\n",
      "F1-Score: 62.214411247803156\n",
      "Confusion Matrix: [[296 246]\n",
      " [184 354]]\n",
      "Accuracy: 98.42592592592592\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1-Score: 0.0\n",
      "Confusion Matrix: [[1063    3]\n",
      " [  14    0]]\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#################### Test Classification #####################\n",
    "##############################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "        \n",
    "def trainModel(text_vector, label, act=\"relu\", solv=\"adam\", l_rate=\"constant\", l_rate_init=0.001, mom=0.9, iter_change=10):\n",
    "    text_train, text_test, label_train, label_test = train_test_split(text_vector, label, test_size=0.30, random_state=101, shuffle=True)\n",
    "\n",
    "    # Model Training (MLPClassifier)\n",
    "    mlp_classifier = MLPClassifier(random_state=1, max_iter=5000, activation=act, solver=solv, learning_rate=l_rate, learning_rate_init=l_rate_init, momentum=mom, n_iter_no_change=iter_change) #increase iter\n",
    "    mlp_classifier.fit(text_train, label_train)\n",
    "\n",
    "    # Testing\n",
    "    test = mlp_classifier.predict(text_test)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(label_test.to_numpy(), test) * 100)\n",
    "    print(\"Recall:\", metrics.recall_score(label_test.to_numpy(), test, average=\"binary\", pos_label=1) * 100)\n",
    "    print(\"Precision:\", metrics.precision_score(label_test.to_numpy(), test, average=\"binary\", pos_label=1) * 100)\n",
    "    print(\"F1-Score:\", metrics.f1_score(label_test.to_numpy(), test, average=\"binary\", pos_label=1) * 100)\n",
    "    print(\"Confusion Matrix:\", metrics.confusion_matrix(label_test.to_numpy(), test))\n",
    "\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"identity\", \"lbfgs\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"identity\", \"sgd\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"identity\", \"adam\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"logistic\", \"lbfgs\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"logistic\", \"sgd\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"logistic\", \"adam\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"tanh\", \"lbfgs\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"tanh\", \"sgd\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"tanh\", \"adam\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"lbfgs\")\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.001) #76,667\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.002) #76,389\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.003) #77,685\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.01) #77,314\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.02) #76,203\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.03) #76,667\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.1) #76,296\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.2) #73,703\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"constant\", 0.3) #73,703\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.001) #65,462\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.002) #68,056\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.003) #70,277\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.01) #75,556\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.02) #76,203\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.03) #76,389\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.1) #77,592\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.2) #76,389\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"invscaling\", 0.3) #74,444\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.001) #76,667\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.002) #76,574\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.003) #77,5\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.01) #77,685\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.02) #76,296\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.03) #76,111\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.04) #76,203\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.05) #77,96\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06) #77,96\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.1) #76,389\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.2) #76,481\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.3) #76,481\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.4) #77,870\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.5) #76,389\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.6) #75,833\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.7) #76,944\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.8) #76,759\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9) #77,962\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 11) #77,962\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 15) #78,240\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20) #78,240 <=== best\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 50) #78,240\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 100) #78,240\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.07) #77,129\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.08) #76,296\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.09) #77,037\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.1) #77,777\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.11) #75,277\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.2) #73,703\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.3) #71,389\n",
    "#trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"adam\")\n",
    "\n",
    "trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "trainModel(combined_text, Posts_Annotated_df[\"Discriminating\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "trainModel(combined_text, Posts_Annotated_df[\"Inappropriate\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "trainModel(combined_text, Posts_Annotated_df[\"OffTopic\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "trainModel(combined_text, Posts_Annotated_df[\"PersonalStories\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "trainModel(combined_text, Posts_Annotated_df[\"PossiblyFeedback\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "trainModel(combined_text, Posts_Annotated_df[\"SentimentNegative\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "trainModel(combined_text, Posts_Annotated_df[\"SentimentNeutral\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "trainModel(combined_text, Posts_Annotated_df[\"SentimentPositive\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649fab74-0566-4cd7-b93e-34193b0e125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create Training data\n",
      "create Classifier\n",
      "Load Posts\n",
      "start processing posts\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n",
      "34.0\n",
      "35.0\n",
      "36.0\n",
      "37.0\n",
      "38.0\n",
      "39.0\n",
      "40.0\n",
      "41.0\n",
      "42.0\n",
      "43.0\n",
      "44.0\n",
      "45.0\n",
      "46.0\n",
      "47.0\n",
      "48.0\n",
      "49.0\n",
      "50.0\n",
      "51.0\n",
      "52.0\n",
      "53.0\n",
      "54.0\n",
      "55.0\n",
      "56.0\n",
      "57.0\n",
      "58.0\n",
      "59.0\n",
      "60.0\n",
      "61.0\n",
      "62.0\n",
      "63.0\n",
      "64.0\n",
      "65.0\n",
      "66.0\n",
      "67.0\n",
      "68.0\n",
      "69.0\n",
      "70.0\n",
      "71.0\n",
      "72.0\n",
      "73.0\n",
      "74.0\n",
      "75.0\n",
      "76.0\n",
      "77.0\n",
      "78.0\n",
      "79.0\n",
      "80.0\n",
      "81.0\n",
      "82.0\n",
      "83.0\n",
      "84.0\n",
      "85.0\n",
      "86.0\n",
      "87.0\n",
      "88.0\n",
      "89.0\n",
      "90.0\n",
      "91.0\n",
      "92.0\n",
      "93.0\n",
      "94.0\n",
      "95.0\n",
      "96.0\n",
      "97.0\n",
      "98.0\n",
      "99.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "###################### Classify Posts ########################\n",
    "##############################################################\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def trainModel(text_vector, label, act=\"relu\", solv=\"adam\", l_rate=\"constant\", l_rate_init=0.001, mom=0.9, iter_change=10):\n",
    "    # Model Training (MLPClassifier)\n",
    "    mlp_classifier = MLPClassifier(random_state=1, max_iter=5000, activation=act, solver=solv, learning_rate=l_rate, learning_rate_init=l_rate_init, momentum=mom, n_iter_no_change=iter_change) #increase iter\n",
    "    mlp_classifier.fit(text_vector, label)\n",
    "    return mlp_classifier\n",
    "\n",
    "def remove_punct_and_numbers(corpus):\n",
    "    #code from https://stackoverflow.com/questions/45375488/how-to-filter-tokens-from-spacy-document\n",
    "    global nlp\n",
    "    indexes = []\n",
    "    doc = nlp(corpus)\n",
    "    for index, token in enumerate(doc):\n",
    "        if (token.pos_  in ('PUNCT', 'NUM', 'SYM')):\n",
    "            indexes.append(index)\n",
    "    np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
    "    np_array = np.delete(np_array, indexes, axis = 0)\n",
    "    doc2 = Doc(doc.vocab, words=[t.text for i, t in enumerate(doc) if i not in indexes])\n",
    "    doc2.from_array([LOWER, POS, ENT_TYPE, IS_ALPHA], np_array)\n",
    "    return doc2\n",
    "\n",
    "def combine_text(headline, body):\n",
    "    if(headline is None):\n",
    "        if(body is None):\n",
    "            return \"\"\n",
    "        else:\n",
    "            return body\n",
    "    else:\n",
    "        if(body is None):\n",
    "            return headline\n",
    "        else:\n",
    "            #drei pünktchen entfernen und text ohne punkt \n",
    "            if headline[-3:] == \"...\":\n",
    "                headline = headline[:-3]\n",
    "            if body[:3] == \"...\":\n",
    "                body = body[3:]\n",
    "            #alternativ auch manchmal zwei pünktchen\n",
    "            if headline[-3:] == \"..\":\n",
    "                headline = headline[:-3]\n",
    "            if body[:3] == \"..\":\n",
    "                body = body[3:]\n",
    "            \n",
    "            #add missing space, to avoid combining words\n",
    "            if headline[-1:] != \" \":\n",
    "                if body[:1] != \" \":\n",
    "                    headline += \" \"\n",
    "            else:\n",
    "                if body[:1] == \" \":\n",
    "                    body = body[1:]\n",
    "            \n",
    "            return headline + body\n",
    "\n",
    "con = sqlite3.connect('corpus.sqlite3')\n",
    "Posts_Annotated_df = pd.read_sql_query(\"SELECT * FROM Posts_Annotated_combined\", con)\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "shape = nlp(\"test\")\n",
    "\n",
    "#create vector representation for all inputs\n",
    "# Posts\n",
    "print(\"create Training data\")\n",
    "combined_text = np.empty((len(Posts_Annotated_df[\"combined_text\"]), shape.vector.shape[0]))\n",
    "pos = 0\n",
    "for text in Posts_Annotated_df[\"combined_text\"]:\n",
    "    combined_text[pos] = nlp(text).vector\n",
    "    pos += 1\n",
    "\n",
    "#erstelle classifier\n",
    "print(\"create Classifier\")\n",
    "ArgumentsUsed = trainModel(combined_text, Posts_Annotated_df[\"ArgumentsUsed\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "Discriminating = trainModel(combined_text, Posts_Annotated_df[\"Discriminating\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "Inappropriate = trainModel(combined_text, Posts_Annotated_df[\"Inappropriate\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "OffTopic = trainModel(combined_text, Posts_Annotated_df[\"OffTopic\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "PersonalStories = trainModel(combined_text, Posts_Annotated_df[\"PersonalStories\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "PossiblyFeedback = trainModel(combined_text, Posts_Annotated_df[\"PossiblyFeedback\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "SentimentNegative = trainModel(combined_text, Posts_Annotated_df[\"SentimentNegative\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "SentimentNeutral = trainModel(combined_text, Posts_Annotated_df[\"SentimentNeutral\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "SentimentPositive = trainModel(combined_text, Posts_Annotated_df[\"SentimentPositive\"], \"relu\", \"sgd\", \"adaptive\", 0.06, 0.9, 20)\n",
    "\n",
    "#lade posts and combine text\n",
    "print(\"Load Posts\")\n",
    "Posts_df = pd.read_sql_query(\"SELECT ID_Post, Status, Headline, Body, PositiveVotes, NegativeVotes FROM Posts WHERE ID_Post NOT IN (SELECT ID_Post FROM Posts_Annotated)\", con)\n",
    "Posts_df.insert(2, \"combined_text\", Posts_df.apply(lambda x: combine_text(x[\"Headline\"], x[\"Body\"]), axis=1))\n",
    "Posts_df.drop(columns=['Headline', 'Body'], inplace=True)\n",
    "\n",
    "# insert statement\n",
    "insert_table = \"INSERT INTO Posts_Annotated_combined ( ID_Post, Status, combined_text, PositiveVotes, NegativeVotes, ArgumentsUsed, Discriminating, Inappropriate, OffTopic, PersonalStories, PossiblyFeedback, SentimentNegative, SentimentNeutral, SentimentPositive ) values ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )\"\n",
    "\n",
    "#iteriere über posts\n",
    "print(\"start processing posts\")\n",
    "cur = con.cursor()\n",
    "for index, row in Posts_df.iterrows():\n",
    "    if (index % 10000) == 0:\n",
    "        # Save (commit) the changes\n",
    "        con.commit()\n",
    "        # close the database connection\n",
    "        con.close()\n",
    "        con = sqlite3.connect('corpus.sqlite3')\n",
    "        cur = con.cursor()\n",
    "        print(index/10000)\n",
    "\n",
    "    #text to vector\n",
    "    doc = remove_punct_and_numbers(row[\"combined_text\"])\n",
    "    text_vec = doc.vector\n",
    "    text_cleaned = \"\".join(token.text_with_ws for token in doc)\n",
    "\n",
    "    #use classifier\n",
    "    ArgumentsUsedType = ArgumentsUsed.predict([text_vec])\n",
    "    DiscriminatingType = Discriminating.predict([text_vec])\n",
    "    InappropriateType = Inappropriate.predict([text_vec])\n",
    "    OffTopicType = OffTopic.predict([text_vec])\n",
    "    PersonalStoriesType = PersonalStories.predict([text_vec])\n",
    "    PossiblyFeedbackType = PossiblyFeedback.predict([text_vec])\n",
    "    SentimentNegativeType = SentimentNegative.predict([text_vec])\n",
    "    SentimentNeutralType = SentimentNeutral.predict([text_vec])\n",
    "    SentimentPositiveType = SentimentPositive.predict([text_vec])\n",
    "\n",
    "    cur.execute(insert_table, [row['ID_Post'], row['Status'], text_cleaned, row['PositiveVotes'], row['NegativeVotes'], ArgumentsUsedType[0], DiscriminatingType[0], InappropriateType[0], OffTopicType[0], PersonalStoriesType[0], PossiblyFeedbackType[0], SentimentNegativeType[0], SentimentNeutralType[0], SentimentPositiveType[0]])\n",
    "\n",
    "# Save (commit) the changes\n",
    "con.commit()\n",
    "\n",
    "# close the database connection\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
